{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from string import lower\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = \"/home/pablo/Downloads/GoldStandard\"\n",
    "files = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstfile = files[0]\n",
    "firstfile = join(mypath, firstfile)\n",
    "myfile = open(firstfile,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['on', 'good', \"o'clock\", 'did', 'Thursday', 'morning', \"n't\", 'feel', 'Arthur', 'eight', 'At', 'very', '.'])\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\"\"\n",
    "dictionary = set(nltk.word_tokenize(sentence))\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\"First Party Collection/Use\": 0, \n",
    "          \"Third Party Sharing/Collection\": 1,\n",
    "          \"User Access, Edit and Deletion\": 2,\n",
    "          \"Data Retention\": 3,\n",
    "          \"Data Security\": 4,\n",
    "          \"International and Specific Audiences\": 5,\n",
    "          \"Do Not Track\": 6,\n",
    "          \"Policy Change\": 7,\n",
    "          \"Other\": 8,\n",
    "          \"User Choice/Control\": 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_vector(label, labels):\n",
    "    \n",
    "    vector = np.zeros((10))\n",
    "    \n",
    "    index = labels[label]\n",
    "    \n",
    "    vector[index] = 1\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_vector(\"First Party Collection/Use\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n",
      "First Party Collection/Use\n",
      "First Party Collection/Use\n",
      "First Party Collection/Use\n",
      "First Party Collection/Use\n",
      "Third Party Sharing/Collection\n"
     ]
    }
   ],
   "source": [
    "f = open(\"/home/pablo/text.txt\",\"w+\")\n",
    "\n",
    "for i, line in enumerate(myfile):\n",
    "    \n",
    "    if i in [0,1,2,3,4,5]:\n",
    "        \n",
    "        a = line.split('\",\"')\n",
    "        \n",
    "        label = a[2].replace('\"',\"\").replace(\"\\n\",\"\")\n",
    "        \n",
    "        print(label)\n",
    "        \n",
    "        f.write('\"'+label+'\"\\n')\n",
    "        \n",
    "        a[0] = a[0].replace('\"',\"\")\n",
    "        \n",
    "        a[1] = map(lower,set(nltk.word_tokenize(a[1])))\n",
    "        \n",
    "        a[2] = label_to_vector(label,labels)\n",
    "        \n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next steps\n",
    "\n",
    "1. We have to go through all the documents in 115-OPP and get the set of words for each document and then convine the sets.\n",
    "2. Once we have the dictionary with all the words we can take all the files and transform the segments into matices using the dictionary. Dimensions = (number of words) x (length of dictionary).\n",
    "3. We also have to check if all the words in our dictionary are present in the glove embedding. Throw Exceptions to check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
